{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734704a-a637-43d8-b57a-c53a7dc330ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "\n",
    "path = \"\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf959b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisySpeech(Dataset):\n",
    "    def __init__(self, path, device='cpu'):\n",
    "        self.path = path\n",
    "        self.device = device\n",
    "        self.__clean_zip__ = 'clean_trainset_28spk_wav.zip'\n",
    "        self.__noisy_zip__ = 'noisy_trainset_28spk_wav.zip'\n",
    "        with zipfile.ZipFile(self.path+self.__clean_zip__, 'r') as clean_zip:\n",
    "            cleanlist = clean_zip.namelist()\n",
    "        self.__clean_wav_list__ = [s for s in cleanlist if s[-4:] == '.wav']\n",
    "        with zipfile.ZipFile(self.path+self.__noisy_zip__, 'r') as noisy_zip:\n",
    "            noisylist = noisy_zip.namelist()\n",
    "        self.__noisy_wav_list__ = [s for s in noisylist if s[-4:] == '.wav']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__noisy_wav_list__)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with zipfile.ZipFile(self.path+self.__clean_zip__, 'r') as clean_zip:\n",
    "            with clean_zip.open(self.__clean_wav_list__[idx]) as clean_wav_file:                \n",
    "                sr, np_clean_audio = scipy.io.wavfile.read(clean_wav_file)        \n",
    "        with zipfile.ZipFile(self.path+self.__noisy_zip__, 'r') as noisy_zip:\n",
    "            with noisy_zip.open(self.__noisy_wav_list__[idx]) as noisy_wav_file:                \n",
    "                sr, np_noisy_audio = scipy.io.wavfile.read(noisy_wav_file)            \n",
    "        return torch.tensor(np_noisy_audio), torch.tensor(np_clean_audio), torch.tensor(sr)\n",
    "\n",
    "def CollateNoisySpeech(itemlist):\n",
    "    buffer_len = 6*48000 # Maximum length is 60 sec at 48kHz\n",
    "    sample_len = buffer_len\n",
    "    noisy_batch, clean_batch = torch.Tensor(0), torch.Tensor(0)\n",
    "    \n",
    "    for noisy, clean, sr in itemlist:\n",
    "        sample_len = min(len(noisy),sample_len)\n",
    "        noisy_padded, clean_padded = torch.zeros(buffer_len), torch.zeros(buffer_len)\n",
    "        noisy_padded[0:sample_len], clean_padded[0:sample_len] = noisy[0:sample_len], clean[0:sample_len]\n",
    "        noisy_batch = torch.cat((noisy_batch, noisy_padded.unsqueeze(0)))\n",
    "        clean_batch = torch.cat((clean_batch, clean_padded.unsqueeze(0)))\n",
    "   \n",
    "    return noisy_batch[:,0:sample_len], clean_batch[:,0:sample_len], sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd4811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, create dataloaders, set parameters\n",
    "dataset = NoisySpeech(path,device=device)\n",
    "_, _, input_samplerate = dataset.__getitem__(0)\n",
    "resample_samplerate = 16000\n",
    "window_length_ms = 30\n",
    "batch_size = 10\n",
    "n_fft = (2*window_length_ms * resample_samplerate) // 2000\n",
    "hop_length = n_fft // 2\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=CollateNoisySpeech)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=CollateNoisySpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42417ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_samplerate    = 16000,\n",
    "        resample_samplerate = 16000,\n",
    "        window_length_ms    = 30\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.resample = torchaudio.transforms.Resample(orig_freq=input_samplerate, new_freq=resample_samplerate)\n",
    "        n_fft = (2*window_length_ms * resample_samplerate) // 2000\n",
    "        hop_length = n_fft // 2\n",
    "        self.spec = torchaudio.transforms.Spectrogram(n_fft=n_fft,power=None,hop_length=hop_length)\n",
    "        self.output_size = (n_fft+2)//2\n",
    "        \n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Resample the input\n",
    "        # Convert to power spectrogram\n",
    "        resampled = self.resample(waveform)\n",
    "        spec = self.spec(resampled)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "        \n",
    "\n",
    "class PostProcessing(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_samplerate   = 16000,\n",
    "        resample_samplerate = 16000,\n",
    "        window_length_ms    = 30\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.resample = torchaudio.transforms.Resample(orig_freq=resample_samplerate, new_freq=output_samplerate)\n",
    "        n_fft = (2*window_length_ms * resample_samplerate) // 2000\n",
    "        hop_length = n_fft // 2\n",
    "        self.invspec = torchaudio.transforms.InverseSpectrogram(n_fft=n_fft,hop_length=hop_length)\n",
    "       \n",
    "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert to power spectrogram\n",
    "        # Resample the output\n",
    "        waveform = self.invspec(spec)\n",
    "        resampled = self.resample(waveform)\n",
    "        \n",
    "        return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6cfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex 2d conv (code from: https://github.com/pheepa/DCUnet/tree/master)\n",
    "class CConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                   out_channels=self.out_channels, \n",
    "                                   kernel_size=self.kernel_size, \n",
    "                                   padding=self.padding, \n",
    "                                   stride=self.stride)\n",
    "        \n",
    "        self.im_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                 out_channels=self.out_channels, \n",
    "                                 kernel_size=self.kernel_size, \n",
    "                                 padding=self.padding, \n",
    "                                 stride=self.stride)\n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "        \n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dea9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex transpose 2d conv (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "class CConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding, \n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, output_size):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        ct_real = self.real_convt(x_real, output_size) - self.im_convt(x_im, output_size)\n",
    "        ct_im = self.im_convt(x_real, output_size) + self.real_convt(x_im, output_size)\n",
    "        \n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e65159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex 2d batch norm (code from: https://github.com/pheepa/DCUnet/tree/master)\n",
    "class CBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)  \n",
    "        \n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6001e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder block (code from: https://github.com/pheepa/DCUnet/tree/master)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "        \n",
    "        return acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192ded8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder block (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x, output_size):\n",
    "        \n",
    "        conved = self.cconvt(x, output_size)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4c57da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Deep Complex U-Net (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "class DCUnet10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    \n",
    "        # downsampling/encoding\n",
    "        self.downsample0 = Encoder(filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45)\n",
    "        self.downsample1 = Encoder(filter_size=(7,5), stride_size=(2,2), in_channels=45, out_channels=90)\n",
    "        self.downsample2 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90)\n",
    "        self.downsample3 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90)\n",
    "        self.downsample4 = Encoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90)\n",
    "        \n",
    "        # upsampling/decoding\n",
    "        self.upsample0 = Decoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90)\n",
    "        self.upsample1 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90)\n",
    "        self.upsample2 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90)\n",
    "        self.upsample3 = Decoder(filter_size=(7,5), stride_size=(2,2), in_channels=180, out_channels=45)\n",
    "        self.upsample4 = Decoder(filter_size=(7,5), stride_size=(2,2), in_channels=90, output_padding=(0,1),\n",
    "                                 out_channels=1, last_layer=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.view_as_real(x.unsqueeze(1))\n",
    "        # downsampling/encoding\n",
    "        d0 = self.downsample0(x)\n",
    "        d1 = self.downsample1(d0) \n",
    "        d2 = self.downsample2(d1)       \n",
    "        d3 = self.downsample3(d2)       \n",
    "        d4 = self.downsample4(d3)\n",
    "        \n",
    "        # upsampling/decoding \n",
    "        u0 = self.upsample0(d4, output_size=d3[..., 0].size())\n",
    "        # skip-connection\n",
    "        c0 = torch.cat((u0, d3), dim=1)\n",
    "        u1 = self.upsample1(c0, output_size=d2[..., 0].size())\n",
    "        c1 = torch.cat((u1, d2), dim=1)\n",
    "        u2 = self.upsample2(c1, output_size=d1[..., 0].size())\n",
    "        c2 = torch.cat((u2, d1), dim=1)\n",
    "        u3 = self.upsample3(c2, output_size=d0[..., 0].size())\n",
    "        c3 = torch.cat((u3, d0), dim=1)\n",
    "        \n",
    "        gains = self.upsample4(c3, output_size=x[..., 0].size())\n",
    "        \n",
    "        # u4 - the mask\n",
    "        estimated_spec = gains * x\n",
    "        \n",
    "        return torch.view_as_complex(estimated_spec).squeeze(1), torch.view_as_complex(gains).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c12cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, dataloader, model, preprocessor, loss_fn, optimizer, epochs=1):\n",
    "    size = len(dataset)\n",
    "    model.train()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (noisy_batch, clean_batch, _) in enumerate(dataloader):\n",
    "            noisy_spec = preprocessor(noisy_batch).to(device)\n",
    "            clean_spec = preprocessor(clean_batch).to(device)\n",
    "            batch_size = noisy_batch.shape[0]\n",
    "\n",
    "            est_clean_spec, _ = model(noisy_spec)\n",
    "\n",
    "            loss = loss_fn(noisy_spec, est_clean_spec, clean_spec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 10 == 0:\n",
    "                torch.save(model, \"DCUnet10.pt\")\n",
    "                curr_time = time.perf_counter()\n",
    "                loss, current = loss.item(), 1 + (batch)*batch_size + epoch*size\n",
    "                print(f\"loss: {loss:>7f} [{current:>5d}/{size*epochs:>5d}] at {curr_time-start_time:>5f} sec\")\n",
    "                start_time = curr_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd9f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8):\n",
    "    # to time-domain waveform\n",
    "    y_true = torch.istft(y_true_, n_fft=n_fft, hop_length=hop_length)\n",
    "    x = torch.istft(x_, n_fft=n_fft, hop_length=hop_length)\n",
    "    y_pred = torch.istft(y_pred_, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    def sdr_fn(true, pred, eps=1e-8):\n",
    "        num = torch.sum(true * pred, dim=1)\n",
    "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "        return -(num / (den + eps))\n",
    "\n",
    "    # true and estimated noise\n",
    "    z_true = x - y_true\n",
    "    z_pred = x - y_pred\n",
    "\n",
    "    a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps)\n",
    "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "    return torch.mean(wSDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing stft not correct padding to max length atm\n",
    "enhancer = DCUnet10().to(device)\n",
    "\n",
    "loss_fn = wsdr_fn\n",
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(enhancer.parameters(), lr=0.001)\n",
    "preprocessor = PreProcessing(input_samplerate=input_samplerate, resample_samplerate=resample_samplerate, window_length_ms=window_length_ms)\n",
    "\n",
    "train(dataset, train_dataloader, enhancer, preprocessor, loss_fn, optimizer=optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f28887",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enhancer, \"DCUnet10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f21e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_batch, clean_batch, sr = next(iter(test_dataloader))\n",
    "noisy_spec = preprocessor(noisy_batch).to(device)\n",
    "clean_spec = preprocessor(clean_batch).to(device)\n",
    "\n",
    "postprocessor = PostProcessing(output_samplerate=input_samplerate)\n",
    "\n",
    "enhancer.eval()\n",
    "with torch.no_grad():\n",
    "    enhanced_spec, gains = enhancer(noisy_spec)\n",
    "\n",
    "enhanced_batch = postprocessor(enhanced_spec.to('cpu'))\n",
    "clean_audio = postprocessor(clean_spec.to('cpu'))\n",
    "noisy_audio = postprocessor(noisy_spec.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8173f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(batch_size)\n",
    "print(idx)\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(noisy_spec[idx,:,:].to('cpu').abs().log().mT.numpy(),origin='lower', aspect=\"auto\")\n",
    "plt.subplot(132)\n",
    "plt.imshow(enhanced_spec[idx,:,:].to('cpu').abs().log().mT.detach().numpy(),origin='lower', aspect=\"auto\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(clean_spec[idx,:,:].to('cpu').abs().log().mT.numpy(),origin='lower', aspect=\"auto\")\n",
    "plt.show()\n",
    "\n",
    "import IPython\n",
    "IPython.display.display(IPython.display.Audio(noisy_batch[idx,:].detach().numpy(),rate=int(sr)))\n",
    "IPython.display.display(IPython.display.Audio(enhanced_batch[idx,:].detach().numpy(),rate=int(sr)))\n",
    "IPython.display.display(IPython.display.Audio(clean_batch[idx,:],rate=int(sr)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
