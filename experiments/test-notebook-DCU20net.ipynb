{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734704a-a637-43d8-b57a-c53a7dc330ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "\n",
    "# dataset path\n",
    "path = \"\"\n",
    "path_encoder = \"\"\n",
    "clean_zip_path = \"clean_trainset_28spk_wav.zip\"\n",
    "noisy_zip_path = \"noisy_trainset_28spk_wav.zip\"\n",
    "extract_dir = \"\"\n",
    "\n",
    "# training params\n",
    "batch_size = 10\n",
    "epochs = 2\n",
    "resample_samplerate = 16000\n",
    "n_fft = 480\n",
    "hop_length = n_fft // 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf959b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisySpeech(Dataset):\n",
    "    def __init__(self, clean_zip_path, noisy_zip_path, extract_dir, device='cpu'):\n",
    "        self.device = device\n",
    "        self.extract_dir = extract_dir\n",
    "        \n",
    "        clean_dir = extract_dir + \"/clean\"\n",
    "        noisy_dir = extract_dir + \"/noisy\"\n",
    "        \n",
    "        # Extract the ZIP files only if the directories don't exist or are empty\n",
    "        if not os.path.exists(clean_dir) or not os.listdir(clean_dir):\n",
    "            print(f\"Extracting clean files to {clean_dir}...\")\n",
    "            with zipfile.ZipFile(clean_zip_path, 'r') as clean_zip:\n",
    "                clean_zip.extractall(clean_dir)\n",
    "\n",
    "        if not os.path.exists(noisy_dir) or not os.listdir(noisy_dir):\n",
    "            print(f\"Extracting noisy files to {noisy_dir}...\")\n",
    "            with zipfile.ZipFile(noisy_zip_path, 'r') as noisy_zip:\n",
    "                noisy_zip.extractall(noisy_dir)\n",
    "        \n",
    "        # Find the actual subdirectories where the .wav files are stored\n",
    "        clean_subdir = os.path.join(clean_dir, os.listdir(clean_dir)[0])  # Gets the first directory inside 'clean'\n",
    "        noisy_subdir = os.path.join(noisy_dir, os.listdir(noisy_dir)[0])  # Gets the first directory inside 'noisy'\n",
    "        \n",
    "        # List all .wav files from the subdirectory\n",
    "        self.__clean_wav_list__ = sorted([os.path.join(clean_subdir, f) \n",
    "                                          for f in os.listdir(clean_subdir) if f.endswith(\".wav\")])\n",
    "        self.__noisy_wav_list__ = sorted([os.path.join(noisy_subdir, f) \n",
    "                                          for f in os.listdir(noisy_subdir) if f.endswith(\".wav\")])\n",
    "        \n",
    "        # Debugging: Print the number of files found\n",
    "        print(f\"Found {len(self.__clean_wav_list__)} clean .wav files\")\n",
    "        print(f\"Found {len(self.__noisy_wav_list__)} noisy .wav files\")\n",
    "\n",
    "        # Ensure that both lists have the same number of files, or use the smaller length\n",
    "        self.dataset_length = min(len(self.__clean_wav_list__), len(self.__noisy_wav_list__))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.dataset_length:\n",
    "            raise IndexError(\"Index out of range for dataset\")\n",
    "\n",
    "        # Directly read the extracted files\n",
    "        sr, np_clean_audio = scipy.io.wavfile.read(self.__clean_wav_list__[idx])\n",
    "        sr, np_noisy_audio = scipy.io.wavfile.read(self.__noisy_wav_list__[idx])\n",
    "        \n",
    "        return torch.tensor(np_noisy_audio).to(self.device), torch.tensor(np_clean_audio).to(self.device), torch.tensor(sr)\n",
    "\n",
    "def CollateNoisySpeech(itemlist):\n",
    "    buffer_len = 6 * 48000  # Maximum length is 60 sec at 48kHz\n",
    "    sample_len = min(min(len(noisy) for noisy, _, _ in itemlist), buffer_len)\n",
    "    \n",
    "    noisy_batch = torch.zeros((len(itemlist), buffer_len))\n",
    "    clean_batch = torch.zeros((len(itemlist), buffer_len))\n",
    "    \n",
    "    for i, (noisy, clean, sr) in enumerate(itemlist):\n",
    "        noisy_batch[i, :sample_len] = noisy[:sample_len]\n",
    "        clean_batch[i, :sample_len] = clean[:sample_len]\n",
    "    \n",
    "    return noisy_batch[:, :sample_len], clean_batch[:, :sample_len], sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacaef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NoisySpeech(clean_zip_path, noisy_zip_path, extract_dir, device='cpu')\n",
    "_, _, input_samplerate = dataset.__getitem__(0)\n",
    "print(input_samplerate)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=CollateNoisySpeech)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=CollateNoisySpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42417ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_samplerate=16000,\n",
    "        resample_samplerate=16000,\n",
    "        n_fft = 512,\n",
    "        hop_length = 512 // 2,\n",
    "        device='cuda'  # Add device parameter\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.resample = torchaudio.transforms.Resample(orig_freq=input_samplerate, new_freq=resample_samplerate).to(self.device)\n",
    "        self.spec = torchaudio.transforms.Spectrogram(n_fft=n_fft, power=None, hop_length=hop_length, window_fn=torch.hann_window, center=True).to(self.device)\n",
    "        self.output_size = (n_fft + 2) // 2\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        # Ensure waveform is on the correct device\n",
    "        waveform = waveform.to(self.device)\n",
    "        \n",
    "        # Resample the input\n",
    "        resampled = self.resample(waveform)\n",
    "        \n",
    "        # Convert to power spectrogram\n",
    "        spec = self.spec(resampled)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "\n",
    "class PostProcessing(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_samplerate=16000,\n",
    "        resample_samplerate=16000,\n",
    "        n_fft = 512,\n",
    "        hop_length = 512 // 2,\n",
    "        device='cuda'  # Add device parameter\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.resample = torchaudio.transforms.Resample(orig_freq=resample_samplerate, new_freq=output_samplerate).to(self.device)\n",
    "        self.invspec = torchaudio.transforms.InverseSpectrogram(n_fft=n_fft, hop_length=hop_length, window_fn=torch.hann_window, center=True).to(self.device)\n",
    "\n",
    "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        # Ensure spec is on the correct device\n",
    "        spec = spec.to(self.device)\n",
    "        \n",
    "        # Convert to waveform from spectrogram\n",
    "        waveform = self.invspec(spec)\n",
    "        \n",
    "        # Resample the output\n",
    "        resampled = self.resample(waveform)\n",
    "        \n",
    "        return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6cfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex 2d conv (code from: https://github.com/pheepa/DCUnet/tree/master)\n",
    "class CConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                   out_channels=self.out_channels, \n",
    "                                   kernel_size=self.kernel_size, \n",
    "                                   padding=self.padding, \n",
    "                                   stride=self.stride)\n",
    "        \n",
    "        self.im_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                 out_channels=self.out_channels, \n",
    "                                 kernel_size=self.kernel_size, \n",
    "                                 padding=self.padding, \n",
    "                                 stride=self.stride)\n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "        \n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dea9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex transpose 2d conv (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "class CConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding, \n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, output_size):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        ct_real = self.real_convt(x_real, output_size) - self.im_convt(x_im, output_size)\n",
    "        ct_im = self.im_convt(x_real, output_size) + self.real_convt(x_im, output_size)\n",
    "        \n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e65159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex 2d batch norm (code from: https://github.com/pheepa/DCUnet/tree/master)\n",
    "class CBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)  \n",
    "        \n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6001e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder block (code from: https://github.com/pheepa/DCUnet/tree/master)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "        \n",
    "        return acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192ded8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder block (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x, output_size):\n",
    "        \n",
    "        conved = self.cconvt(x, output_size)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86bfbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Deep Complex U-Net (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "class DCUnet20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # downsampling/encoding\n",
    "        self.downsample0 = Encoder(filter_size=(7,1), stride_size=(1,1), in_channels=1, out_channels=45, padding=(3,0))\n",
    "        self.downsample1 = Encoder(filter_size=(1,7), stride_size=(1,1), in_channels=45, out_channels=45, padding=(0,3))\n",
    "        self.downsample2 = Encoder(filter_size=(7,5), stride_size=(2,2), in_channels=45, out_channels=90, padding=(3,2))\n",
    "        self.downsample3 = Encoder(filter_size=(7,5), stride_size=(2,1), in_channels=90, out_channels=90, padding=(3,2))\n",
    "        self.downsample4 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90, padding=(2,1))\n",
    "        self.downsample5 = Encoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90, padding=(2,1))\n",
    "        self.downsample6 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90, padding=(2,1))\n",
    "        self.downsample7 = Encoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=90, padding=(2,1))\n",
    "        self.downsample8 = Encoder(filter_size=(5,3), stride_size=(2,2), in_channels=90, out_channels=90, padding=(2,1))\n",
    "        self.downsample9 = Encoder(filter_size=(5,3), stride_size=(2,1), in_channels=90, out_channels=128, padding=(2,1))\n",
    "        \n",
    "        # upsampling/decoding\n",
    "        self.upsample0 = Decoder(filter_size=(5,3), stride_size=(2,1), in_channels=128, out_channels=90, padding=(2,1))\n",
    "        self.upsample1 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90, padding=(2,1))\n",
    "        self.upsample2 = Decoder(filter_size=(5,3), stride_size=(2,1), in_channels=180, out_channels=90, padding=(2,1))\n",
    "        self.upsample3 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90, padding=(2,1))\n",
    "        self.upsample4 = Decoder(filter_size=(5,3), stride_size=(2,1), in_channels=180, out_channels=90, padding=(2,1))\n",
    "        self.upsample5 = Decoder(filter_size=(5,3), stride_size=(2,2), in_channels=180, out_channels=90, padding=(2,1))\n",
    "        self.upsample6 = Decoder(filter_size=(7,5), stride_size=(2,1), in_channels=180, out_channels=90, padding=(3,2))\n",
    "        self.upsample7 = Decoder(filter_size=(7,5), stride_size=(2,2), in_channels=180, out_channels=90, padding=(3,2))\n",
    "        self.upsample8 = Decoder(filter_size=(1,7), stride_size=(1,1), in_channels=135, out_channels=90, padding=(0,3))\n",
    "        self.upsample9 = Decoder(filter_size=(7,1), stride_size=(1,1), in_channels=135, output_padding=(0,1), padding=(3,0),\n",
    "                                 out_channels=1, last_layer=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.view_as_real(x.unsqueeze(1))\n",
    "        # downsampling/encoding\n",
    "        d0 = self.downsample0(x)\n",
    "        d1 = self.downsample1(d0)\n",
    "        d2 = self.downsample2(d1)     \n",
    "        d3 = self.downsample3(d2) \n",
    "        d4 = self.downsample4(d3)\n",
    "        d5 = self.downsample5(d4)\n",
    "        d6 = self.downsample6(d5)\n",
    "        d7 = self.downsample7(d6)\n",
    "        d8 = self.downsample8(d7)\n",
    "        d9 = self.downsample9(d8)\n",
    "        \n",
    "        # upsampling/decoding \n",
    "        u0 = self.upsample0(d9, output_size=d8[..., 0].size())\n",
    "        # skip-connection\n",
    "        c0 = torch.cat((u0, d8), dim=1)\n",
    "\n",
    "        u1 = self.upsample1(c0, output_size=d7[..., 0].size())\n",
    "        c1 = torch.cat((u1, d7), dim=1)\n",
    "\n",
    "        u2 = self.upsample2(c1, output_size=d6[..., 0].size())\n",
    "        c2 = torch.cat((u2, d6), dim=1)\n",
    "\n",
    "        u3 = self.upsample3(c2, output_size=d5[..., 0].size())\n",
    "        c3 = torch.cat((u3, d5), dim=1)\n",
    "\n",
    "        u4 = self.upsample4(c3, output_size=d4[..., 0].size())\n",
    "        c4 = torch.cat((u4, d4), dim=1)\n",
    "\n",
    "        u5 = self.upsample5(c4, output_size=d3[..., 0].size())\n",
    "        c5 = torch.cat((u5, d3), dim=1)\n",
    "\n",
    "        u6 = self.upsample6(c5, output_size=d2[..., 0].size())\n",
    "        c6 = torch.cat((u6, d2), dim=1)\n",
    "\n",
    "        u7 = self.upsample7(c6, output_size=d1[..., 0].size())\n",
    "        c7 = torch.cat((u7, d1), dim=1)\n",
    "    \n",
    "        u8 = self.upsample8(c7, output_size=d1[..., 0].size())\n",
    "        c8 = torch.cat((u8, d0), dim=1)\n",
    "        \n",
    "        gains = self.upsample9(c8, output_size=x[..., 0].size())\n",
    "        \n",
    "        # u4 - the mask\n",
    "        estimated_spec = gains * x\n",
    "        \n",
    "        return torch.view_as_complex(estimated_spec).squeeze(1), torch.view_as_complex(gains).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c12cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, dataloader, model, preprocessor, loss_fn, optimizer, epochs=1):\n",
    "    size = len(dataset)\n",
    "    model.train()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (noisy_batch, clean_batch, _) in enumerate(dataloader):\n",
    "            noisy_spec = preprocessor(noisy_batch).to(device)\n",
    "            clean_spec = preprocessor(clean_batch).to(device)\n",
    "\n",
    "            est_clean_spec, _ = model(noisy_spec)\n",
    "\n",
    "            loss = loss_fn(noisy_spec, est_clean_spec, clean_spec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch+1) % 10 == 0:\n",
    "                torch.save(model.state_dict(), path_encoder+\"DCUnet20_train.pt\")\n",
    "                curr_time = time.perf_counter()\n",
    "                batch_size = noisy_batch.shape[0]\n",
    "                loss, current = loss.item(), 1 + (batch)*batch_size + epoch*size\n",
    "                print(f\"loss: {loss:>7f} [{current:>5d}/{size*epochs:>5d}] at {curr_time-start_time:>5f} sec\")\n",
    "                start_time = curr_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd9f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (code from: https://github.com/pheepa/DCUnet/tree/master), modified\n",
    "class wsdr_fn(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_fft,\n",
    "            hop_length,\n",
    "            eps=1e-8,\n",
    "            device=\"cuda\"\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.invspec = torchaudio.transforms.InverseSpectrogram(n_fft=n_fft, hop_length=hop_length, window_fn=torch.hann_window, center=True).to(device)\n",
    "            self.eps = eps\n",
    "    \n",
    "    def forward(self, x_, y_pred_, y_true_: torch.Tensor) -> torch.Tensor:\n",
    "        # to time-domain waveform\n",
    "        y_true = self.invspec(y_true_)\n",
    "        x = self.invspec(x_)\n",
    "        y_pred = self.invspec(y_pred_)\n",
    "\n",
    "        def sdr_fn(true, pred, eps=self.eps):\n",
    "            num = torch.sum(true * pred, dim=1)\n",
    "            den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "            return -(num / (den + eps))\n",
    "\n",
    "        # true and estimated noise\n",
    "        z_true = x - y_true\n",
    "        z_pred = x - y_pred\n",
    "\n",
    "        a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + self.eps)\n",
    "        wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "        return torch.mean(wSDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing stft not correct padding to max length atm\n",
    "enhancer = DCUnet20().to(device)\n",
    "\n",
    "loss_fn = wsdr_fn(n_fft=n_fft, hop_length=hop_length, eps=1e-8, device=device)\n",
    "optimizer = torch.optim.Adam(enhancer.parameters(), lr=0.001)\n",
    "preprocessor = PreProcessing(input_samplerate=input_samplerate, resample_samplerate=resample_samplerate, n_fft=n_fft, hop_length=hop_length, device=device)\n",
    "\n",
    "train(dataset, train_dataloader, enhancer, preprocessor, loss_fn, optimizer=optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f28887",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enhancer.state_dict(), path_encoder+\"DCUnet20.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhancer = DCUnet20()\n",
    "enhancer.load_state_dict(torch.load(path_encoder+\"DCUnet20.pt\", weights_only=True))\n",
    "enhancer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87f21e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_batch, clean_batch, sr = next(iter(test_dataloader))\n",
    "\n",
    "preprocessor = PreProcessing(input_samplerate=input_samplerate, resample_samplerate=resample_samplerate, n_fft=n_fft, hop_length=hop_length, device=device)\n",
    "postprocessor = PostProcessing(output_samplerate=input_samplerate, resample_samplerate=resample_samplerate, n_fft=n_fft, hop_length=hop_length, device=device)\n",
    "\n",
    "noisy_spec = preprocessor(noisy_batch).to(device)\n",
    "clean_spec = preprocessor(clean_batch).to(device)\n",
    "\n",
    "enhancer.eval()\n",
    "with torch.no_grad():\n",
    "    enhanced_spec, gains = enhancer(noisy_spec)\n",
    "\n",
    "enhanced_batch = postprocessor(enhanced_spec).to('cpu')\n",
    "clean_audio = postprocessor(clean_spec).to('cpu')\n",
    "noisy_audio = postprocessor(noisy_spec).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8173f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(batch_size)\n",
    "print(idx)\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(noisy_spec[idx,:,:].to('cpu').abs().log().numpy(),origin='lower', aspect=\"auto\")\n",
    "plt.subplot(132)\n",
    "plt.imshow(enhanced_spec[idx,:,:].to('cpu').abs().log().detach().numpy(),origin='lower', aspect=\"auto\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(clean_spec[idx,:,:].to('cpu').abs().log().numpy(),origin='lower', aspect=\"auto\")\n",
    "plt.show()\n",
    "\n",
    "import IPython\n",
    "IPython.display.display(IPython.display.Audio(noisy_batch[idx,:].detach().numpy(),rate=int(sr)))\n",
    "IPython.display.display(IPython.display.Audio(enhanced_batch[idx,:].detach().numpy(),rate=int(sr)))\n",
    "IPython.display.display(IPython.display.Audio(clean_batch[idx,:],rate=int(sr)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
